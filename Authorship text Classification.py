# -*- coding: utf-8 -*-
"""Untitled29.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SmZ6a6XoePvwgGamcb0S86Cr0TloQ7hx
"""

import re
import math
from collections import defaultdict, Counter
import random # Kept for potential text generation later

# --- Data Definition (For Context) ---
AUTHOR_A_NAME = "Author A (Shakespeare Style)"
AUTHOR_B_NAME = "Author B (Simple Prose Style)"

corpus_a = """
To be, or not to be, that is the question:
Whether 'tis nobler in the mind to suffer
The slings and arrows of outrageous fortune,
Or to take arms against a sea of troubles,
And by opposing end them. This pale cast of thought
Hath ever been the subject of thy careful study.
Hark, what light through yonder window breaks? It is the east, and Juliet is the sun.
Go forth and seek thy fate, for destiny awaits thee.
"""

corpus_b = """
The sun was bright and the water was cold. I stood on the bank of the river and fished.
There was no wind, and the boat stayed still. He needed money for the trip, but the bank was closed.
It was a good day to read the paper and drink a cup of coffee.
We have to go home now. We will see the man tomorrow. The work is finished.
"""
test_text_4 = "To be, or not to be, that is the question"
# ----------------------------------------

# --- STEP 1: TOKENIZATION (Fixed: Added BOS/EOS for boundaries) ---
def simple_tokenize(text):
    """
    Tokenizes text, converts to lowercase, and adds boundary markers (<s>, </s>).
    FIXED: Resolves the instructor's feedback on consistency and boundary tokens.
    """
    # Robust regex to split on punctuation and whitespace
    parts = re.split(r'([,.:;?_!"()\[\]{}&@\\]|--|\s)', text.lower())
    # Filter out empty strings and strings containing only whitespace
    tokens = [i.strip() for i in parts if i and i.strip()]

    # Structure text into sentences with boundary markers
    sentences = []
    current_sentence = ['<s>']

    for token in tokens:
        current_sentence.append(token)
        # Define sentence-ending punctuation
        if token in ['.', '?', '!']:
            current_sentence.append('</s>')
            sentences.append(current_sentence)
            current_sentence = ['<s>']

    # If the text doesn't end neatly (e.g., just a single phrase)
    if len(current_sentence) > 1:
        current_sentence.append('</s>')
        sentences.append(current_sentence)

    # Flatten the list of lists into a single token stream
    return [token for sentence in sentences for token in sentence]

# --- STEP 2: COUNTER (Fixed: Takes tokens, not raw text) ---
def counter(tokens):
    """
    Counts unigrams and bigrams from a list of tokens.
    FIXED: Resolves the major bug of counting characters instead of words.
    """
    # FIX: Input is now 'tokens' (list of words)
    uni = Counter(tokens)
    bi = Counter(zip(tokens[:-1], tokens[1:]))
    return uni, bi

# --- STEP 3: LAPLACE PROBABILITIES (User's function) ---
def laplace_bigram_probs(tokens, uni, bi):
    """
    Build unigram and bigram counts and return a smoothed bigram distribution (P_lap).
    Uses Add-One (Laplace) Smoothing: P(w2|w1) = (Count(w1, w2) + 1) / (Count(w1) + V)
    """
    # V is the size of the vocabulary (all unique tokens)
    V_set = set(tokens)
    V = len(V_set)
    P_lap = defaultdict(dict)

    for w1 in V_set:
        # Denominator: Count(w1) + V (since alpha=1)
        denom = uni.get(w1, 0) + V

        for w2 in V_set:
            # Numerator: Count(w1, w2) + 1
            c12 = bi.get((w1, w2), 0)
            P_lap[w1][w2] = (c12 + 1) / denom

    return P_lap, V # Return the smoothed dict and V

# --- STEP 4: MODEL WRAPPER ---
def model(text):
    """
    Orchestrates the training process: tokenization, counting, and Laplace smoothing.
    """
    # Consistent preprocessing
    tokens = simple_tokenize(text)

    # Get counts
    uni, bi = counter(tokens)

    # Get smoothed probabilities
    P_lap, V = laplace_bigram_probs(tokens, uni, bi)

    # Returns the smoothed probability map and the vocabulary size V
    return P_lap, V

# --- STEP 5: SCORE CALCULATION (Fixed: Uses log, P_lap, and length normalization) ---
def calculate_text_prob_score(text, P_lap, V):
    """
    Calculates the average log probability (log-likelihood) per bigram transition.
    FIXED: Uses log() and length normalization.
    """
    # Test text MUST be tokenized with BOS/EOS, identical to training
    tokens = simple_tokenize(text)

    prob_score = 0.0 # This variable holds the total LOG probability sum
    num_bigrams = 0

    for i in range(len(tokens) - 1):
        w1, w2 = tokens[i], tokens[i+1]

        prob = 1e-10 # Default tiny smoothing for truly unseen tokens/bigrams (safety net)

        # Check if w1 is in the model (it should be, if P_lap covers V)
        if w1 in P_lap:
            # Check if the specific bigram is in the pre-smoothed distribution
            if w2 in P_lap[w1]:
                prob = P_lap[w1][w2]
            else:
                 # This case means w2 is Out-of-Vocabulary (OOV) relative to the training V.
                 # The Add-One smoothing already handled OOV in training by assigning P=1/denom to all
                 # *potential* V x V transitions. However, if w2 is truly OOV in a practical sense,
                 # its probability will be very low or require special handling.
                 # Since P_lap was built on all tokens in the training data, if w2 is new, it won't be in P_lap[w1].
                 # We rely on the implicit V+1 smoothing done by P_lap, but for safety, if w1 is known but w2 is OOV,
                 # we use a highly conservative estimate, though strictly speaking, P_lap should cover all possibilities.
                 # We will use the smallest probability assigned to any transition involving w1.
                 prob = 1 / (len(P_lap) + V) # Very safe, tiny probability

        # Add the log of the probability
        prob_score += math.log(prob)
        num_bigrams += 1

    # Length Normalization: Return the average log probability per transition
    if num_bigrams == 0:
        return 0.0

    return prob_score / num_bigrams

# --- STEP 6: CLASSIFICATION (Finalized) ---
def classify_text(text, model_A, model_B):
    """
    Classifies a text by comparing its length-normalized log probability under two models.
    """
    # Unpack the models (P_lap, V)
    P_A, V_A = model_A
    P_B, V_B = model_B

    # Calculate the average log probability under each model
    prob_score_A = calculate_text_prob_score(text, P_A, V_A)
    prob_score_B = calculate_text_prob_score(text, P_B, V_B)

    print("-" * 50)
    print(f"Text to Classify: \"{text}\"")
    print(f"Normalized Log-Likelihood ({AUTHOR_A_NAME}): {prob_score_A:.6f}")
    print(f"Normalized Log-Likelihood ({AUTHOR_B_NAME}): {prob_score_B:.6f}")

    if prob_score_A > prob_score_B:
        prediction = AUTHOR_A_NAME
        score_diff = prob_score_A - prob_score_B
    elif prob_score_B > prob_score_A:
        prediction = AUTHOR_B_NAME
        score_diff = prob_score_B - prob_score_A
    else:
        prediction = "Undetermined (Scores are equal)"
        score_diff = 0.0

    print(f"Classification: {prediction} (Normalized Score Diff: {abs(score_diff):.6f})")
    print("-" * 50)
    return prediction

# ===============================================
# --- MAIN EXECUTION ---
# ===============================================

# 1. Train Models
print("--- Training Models with Laplace Smoothing ---")

# Train A
model_A = model(corpus_a)

# Train B
model_B = model(corpus_b)

# The model tuple is (P_lap, V)
P_A, V_A = model_A
P_B, V_B = model_B

print(f"{AUTHOR_A_NAME} Vocabulary Size (V_A): {V_A}")
print(f"{AUTHOR_B_NAME} Vocabulary Size (V_B): {V_B}")

# 2. Classification
print("\n=============== CLASSIFICATION RESULTS ===============")

# Use the specific test text from the instructor's feedback
classify_text(test_text_4, model_A, model_B)